---
phase: 11-foraging-specialization
plan: 03
type: execute
wave: 2
depends_on: ["11-02"]
files_modified:
  - bin/cli.js
autonomous: true

must_haves:
  truths:
    - "User can run 'aether telemetry' to view overall summary"
    - "User can run 'aether telemetry model <name>' to see per-model stats"
    - "User can run 'aether telemetry performance' to see ranked model performance"
    - "CLI output is formatted with colors and clear labels"
  artifacts:
    - path: "bin/cli.js"
      provides: "Telemetry CLI commands"
      adds_commands: ["telemetry", "telemetry model", "telemetry performance"]
  key_links:
    - from: "CLI telemetry command"
      to: "telemetry.js query functions"
      via: "require('./lib/telemetry')"
      pattern: "getTelemetrySummary, getModelPerformance"
---

<objective>
Add CLI commands for viewing telemetry data and model performance.

Purpose: Enable users to query and analyze model performance data, fulfilling MOD-07 requirement for viewing telemetry.
Output: Extended bin/cli.js with `aether telemetry` command and subcommands.
</objective>

<execution_context>
@~/.claude/cosmic-dev-system/workflows/execute-plan.md
@~/.claude/cosmic-dev-system/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/11-foraging-specialization/11-RESEARCH.md
@/Users/callumcowie/repos/Aether/bin/cli.js
@/Users/callumcowie/repos/Aether/bin/lib/telemetry.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add telemetry command to CLI</name>
  <files>bin/cli.js</files>
  <action>
Add a new `telemetry` command to the CLI program in bin/cli.js:

1. Import telemetry functions at the top (after existing imports):
   ```javascript
   const {
     getTelemetrySummary,
     getModelPerformance,
   } = require('./lib/telemetry');
   ```

2. Create the telemetry command with subcommands:
   ```javascript
   const telemetryCmd = program
     .command('telemetry')
     .description('View model performance telemetry');
   ```

3. Add subcommand `summary` (default):
   ```javascript
   telemetryCmd
     .command('summary')
     .description('Show overall telemetry summary')
     .action(wrapCommand(async () => {
       const repoPath = process.cwd();
       const summary = getTelemetrySummary(repoPath);

       console.log(c.header('Model Performance Telemetry\n'));
       console.log(`Total Spawns: ${summary.total_spawns}`);
       console.log(`Models Used: ${summary.total_models}\n`);

       if (summary.total_spawns === 0) {
         console.log(c.info('No telemetry data yet. Run some builds to collect data.'));
         return;
       }

       console.log('Model Performance:');
       console.log('â”€'.repeat(60));
       for (const [model, stats] of Object.entries(summary.models)) {
         const rate = (stats.success_rate * 100).toFixed(1);
         const rateColor = stats.success_rate >= 0.9 ? c.success :
                          stats.success_rate >= 0.7 ? c.warning : c.error;
         console.log(`  ${model.padEnd(15)} ${String(stats.total_spawns).padStart(4)} spawns  ${rateColor(rate + '%')} success`);
       }

       if (summary.recent_decisions.length > 0) {
         console.log('\nRecent Routing Decisions:');
         console.log('â”€'.repeat(60));
         for (const decision of summary.recent_decisions.slice(-5)) {
           console.log(`  ${decision.caste.padEnd(10)} â†’ ${decision.selected_model.padEnd(12)} (${decision.source})`);
         }
       }
     }));
   ```

4. Make `summary` the default action when running `aether telemetry` without subcommand.
  </action>
  <verify>grep -q "command('telemetry')" bin/cli.js && grep -q "getTelemetrySummary" bin/cli.js && echo "Telemetry command added"</verify>
  <done>Telemetry command with summary subcommand exists in CLI</done>
</task>

<task type="auto">
  <name>Task 2: Add model and performance subcommands</name>
  <files>bin/cli.js</files>
  <action>
Add additional telemetry subcommands:

1. **model subcommand** - Show detailed stats for a specific model:
   ```javascript
   telemetryCmd
     .command('model <model-name>')
     .description('Show detailed performance for a specific model')
     .action(wrapCommand(async (modelName) => {
       const repoPath = process.cwd();
       const performance = getModelPerformance(repoPath, modelName);

       if (!performance) {
         console.log(c.warning(`No data for model: ${modelName}`));
         return;
       }

       console.log(c.header(`Model Performance: ${modelName}\n`));
       console.log(`Total Spawns: ${performance.total_spawns}`);
       console.log(`Success Rate: ${(performance.success_rate * 100).toFixed(1)}%`);
       console.log(`  âœ“ Completed: ${performance.successful_completions}`);
       console.log(`  âœ— Failed: ${performance.failed_completions}`);
       console.log(`  ðŸš« Blocked: ${performance.blocked}`);

       if (Object.keys(performance.by_caste).length > 0) {
         console.log('\nPerformance by Caste:');
         console.log('â”€'.repeat(50));
         for (const [caste, stats] of Object.entries(performance.by_caste)) {
           const casteRate = stats.spawns > 0 ? (stats.success / stats.spawns * 100).toFixed(1) : '0.0';
           console.log(`  ${caste.padEnd(12)} ${String(stats.spawns).padStart(4)} spawns  ${casteRate}% success`);
         }
       }
     }));
   ```

2. **performance subcommand** - Show ranked performance table:
   ```javascript
   telemetryCmd
     .command('performance')
     .description('Show models ranked by performance')
     .action(wrapCommand(async () => {
       const repoPath = process.cwd();
       const summary = getTelemetrySummary(repoPath);

       console.log(c.header('Model Performance Ranking\n'));

       if (summary.total_spawns === 0) {
         console.log(c.info('No telemetry data yet. Run some builds to collect data.'));
         return;
       }

       // Sort models by success rate
       const ranked = Object.entries(summary.models)
         .map(([model, stats]) => ({ model, ...stats }))
         .sort((a, b) => b.success_rate - a.success_rate);

       console.log(`${'Rank'.padEnd(6)} ${'Model'.padEnd(15)} ${'Spawns'.padStart(6)} ${'Success'.padStart(8)} ${'Rate'.padStart(6)}`);
       console.log('â”€'.repeat(60));

       ranked.forEach((m, i) => {
         const rank = `${i + 1}.`.padEnd(6);
         const rate = (m.success_rate * 100).toFixed(1);
         const rateColor = m.success_rate >= 0.9 ? c.success :
                          m.success_rate >= 0.7 ? c.warning : c.error;
         console.log(`${rank} ${m.model.padEnd(15)} ${String(m.total_spawns).padStart(6)} ${String(m.successful_completions || 0).padStart(8)} ${rateColor(rate.padStart(5) + '%')}`);
       });

       console.log('\n' + c.dim('Tip: Use "aether telemetry model <name>" for detailed stats'));
     }));
   ```

3. Update the custom help handler to include telemetry commands.
  </action>
  <verify>grep -q "command('model" bin/cli.js && grep -q "command('performance')" bin/cli.js && echo "Subcommands added"</verify>
  <done>Model and performance subcommands are implemented</done>
</task>

<task type="auto">
  <name>Task 3: Add telemetry tests</name>
  <files>test/cli-telemetry.test.js</files>
  <action>
Create unit tests for the telemetry CLI commands:

1. **telemetry summary tests:**
   - Shows message when no data exists
   - Displays correct total spawns count
   - Lists all models with stats
   - Shows recent routing decisions
   - Uses correct colors for success rates

2. **telemetry model tests:**
   - Shows warning for unknown model
   - Displays detailed stats for valid model
   - Shows breakdown by caste
   - Calculates success rate correctly

3. **telemetry performance tests:**
   - Shows message when no data
   - Ranks models by success rate
   - Displays all columns correctly
   - Uses color coding appropriately

4. **Integration tests:**
   - Commands work with real telemetry data
   - Output format matches expectations

Use the existing CLI test patterns from the codebase. Mock telemetry data for consistent tests.
  </action>
  <verify>npm test -- test/cli-telemetry.test.js 2>&1 | grep -E "(passed|failed)" | tail -1</verify>
  <done>CLI telemetry tests pass</done>
</task>

</tasks>

<verification>
1. Run tests: npm test -- test/cli-telemetry.test.js
2. Manual test: `aether telemetry` (should show summary or "no data" message)
3. Manual test: `aether telemetry performance` (should show ranking table)
4. Manual test: `aether telemetry model kimi-k2.5` (should show model details)
5. Verify help text: `aether --help` shows telemetry command
</verification>

<success_criteria>
- [ ] `aether telemetry` shows overall summary
- [ ] `aether telemetry model <name>` shows detailed model stats
- [ ] `aether telemetry performance` shows ranked performance table
- [ ] Output uses colors appropriately (green >90%, yellow 70-90%, red <70%)
- [ ] Help text includes telemetry commands
- [ ] All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/11-foraging-specialization/11-03-SUMMARY.md`
</output>
